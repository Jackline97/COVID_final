{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import bs4\n",
    "import re\n",
    "import socket\n",
    "import whois\n",
    "from datetime import datetime\n",
    "import time\n",
    "from googlesearch import search\n",
    "import sys\n",
    "from patterns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv4_pattern = r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n",
    "ipv6_pattern = r\"^(?:(?:(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):){6})(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))|(?:(?:(?:(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9]))\\.){3}\" \\\n",
    "               r\"(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9])))))))|(?:(?:::(?:(?:(?:[0-9a-fA-F]{1,4})):){5})\" \\\n",
    "               r\"(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):(?:(?:[0-9a-fA-F]{1,4})))|(?:(?:(?:(?:(?:25[0-5]|\" \\\n",
    "               r\"(?:[1-9]|1[0-9]|2[0-4])?[0-9]))\\.){3}(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9])))))))|\" \\\n",
    "               r\"(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})))?::(?:(?:(?:[0-9a-fA-F]{1,4})):){4})\" \\\n",
    "               r\"(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):(?:(?:[0-9a-fA-F]{1,4})))|(?:(?:(?:(?:(?:25[0-5]|\" \\\n",
    "               r\"(?:[1-9]|1[0-9]|2[0-4])?[0-9]))\\.){3}(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9])))))))|\" \\\n",
    "               r\"(?:(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):){0,1}(?:(?:[0-9a-fA-F]{1,4})))?::\" \\\n",
    "               r\"(?:(?:(?:[0-9a-fA-F]{1,4})):){3})(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):(?:(?:[0-9a-fA-F]{1,4})))|\" \\\n",
    "               r\"(?:(?:(?:(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9]))\\.){3}\" \\\n",
    "               r\"(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9])))))))|(?:(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):){0,2}\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))?::(?:(?:(?:[0-9a-fA-F]{1,4})):){2})(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))|(?:(?:(?:(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9]))\\.){3}(?:(?:25[0-5]|\" \\\n",
    "               r\"(?:[1-9]|1[0-9]|2[0-4])?[0-9])))))))|(?:(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):){0,3}\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))?::(?:(?:[0-9a-fA-F]{1,4})):)(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))|(?:(?:(?:(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9]))\\.){3}\" \\\n",
    "               r\"(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9])))))))|(?:(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):){0,4}\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))?::)(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):(?:(?:[0-9a-fA-F]{1,4})))|\" \\\n",
    "               r\"(?:(?:(?:(?:(?:25[0-5]|(?:[1-9]|1[0-9]|2[0-4])?[0-9]))\\.){3}(?:(?:25[0-5]|\" \\\n",
    "               r\"(?:[1-9]|1[0-9]|2[0-4])?[0-9])))))))|(?:(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):){0,5}\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))?::)(?:(?:[0-9a-fA-F]{1,4})))|(?:(?:(?:(?:(?:(?:[0-9a-fA-F]{1,4})):){0,6}\" \\\n",
    "               r\"(?:(?:[0-9a-fA-F]{1,4})))?::))))$\"\n",
    "shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
    "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
    "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
    "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
    "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
    "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
    "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
    "                      r\"tr\\.im|link\\.zip\\.net\"\n",
    "http_https = r\"https://|http://\"\n",
    "\n",
    "\n",
    "\n",
    "def having_ip_address(url):\n",
    "    ip_address_pattern = ipv4_pattern + \"|\" + ipv6_pattern\n",
    "    match = re.search(ip_address_pattern, url)\n",
    "    return -1 if match else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def url_length(url):\n",
    "    if len(url) < 54:\n",
    "        return 1\n",
    "    if 54 <= len(url) <= 75:\n",
    "        return 0\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortening_service(url):\n",
    "    match = re.search(shortening_services, url)\n",
    "    return -1 if match else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCALHOST_PATH = \"/Library/WebServer/Documents/\"\n",
    "DIRECTORY_NAME = \"Malicious-Web-Content-Detection-Using-Machine-Learning\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def having_at_symbol(url):\n",
    "    match = re.search('@', url)\n",
    "    return -1 if match else 1\n",
    "\n",
    "\n",
    "def double_slash_redirecting(url):\n",
    "    # since the position starts from 0, we have given 6 and not 7 which is according to the document.\n",
    "    # It is convenient and easier to just use string search here to search the last occurrence instead of re.\n",
    "    last_double_slash = url.rfind('//')\n",
    "    return -1 if last_double_slash > 6 else 1\n",
    "\n",
    "\n",
    "def prefix_suffix(domain):\n",
    "    match = re.search('-', domain)\n",
    "    return -1 if match else 1\n",
    "\n",
    "\n",
    "def having_sub_domain(url):\n",
    "    # Here, instead of greater than 1 we will take greater than 3 since the greater than 1 condition is when www and\n",
    "    # country domain dots are skipped\n",
    "    # Accordingly other dots will increase by 1\n",
    "    if having_ip_address(url) == -1:\n",
    "        match = re.search(\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5]))|(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',\n",
    "            url)\n",
    "        pos = match.end()\n",
    "        url = url[pos:]\n",
    "    num_dots = [x.start() for x in re.finditer(r'\\.', url)]\n",
    "    if len(num_dots) <= 3:\n",
    "        return 1\n",
    "    elif len(num_dots) == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def domain_registration_length(domain):\n",
    "    expiration_date = domain.expiration_date\n",
    "    today = time.strftime('%Y-%m-%d')\n",
    "    today = datetime.strptime(today, '%Y-%m-%d')\n",
    "\n",
    "    registration_length = 0\n",
    "    # Some domains do not have expiration dates. This if condition makes sure that the expiration date is used only\n",
    "    # when it is present.\n",
    "    if expiration_date:\n",
    "        registration_length = abs((expiration_date - today).days)\n",
    "    return -1 if registration_length / 365 <= 1 else 1\n",
    "\n",
    "\n",
    "def favicon(wiki, soup, domain):\n",
    "    for head in soup.find_all('head'):\n",
    "        for head.link in soup.find_all('link', href=True):\n",
    "            dots = [x.start() for x in re.finditer(r'\\.', head.link['href'])]\n",
    "            return 1 if wiki in head.link['href'] or len(dots) == 1 or domain in head.link['href'] else -1\n",
    "    return 1\n",
    "\n",
    "\n",
    "def https_token(url):\n",
    "    match = re.search(http_https, url)\n",
    "    if match and match.start() == 0:\n",
    "        url = url[match.end():]\n",
    "    match = re.search('http|https', url)\n",
    "    return -1 if match else 1\n",
    "\n",
    "\n",
    "def request_url(wiki, soup, domain):\n",
    "    i = 0\n",
    "    success = 0\n",
    "    for img in soup.find_all('img', src=True):\n",
    "        dots = [x.start() for x in re.finditer(r'\\.', img['src'])]\n",
    "        if wiki in img['src'] or domain in img['src'] or len(dots) == 1:\n",
    "            success = success + 1\n",
    "        i = i + 1\n",
    "\n",
    "    for audio in soup.find_all('audio', src=True):\n",
    "        dots = [x.start() for x in re.finditer(r'\\.', audio['src'])]\n",
    "        if wiki in audio['src'] or domain in audio['src'] or len(dots) == 1:\n",
    "            success = success + 1\n",
    "        i = i + 1\n",
    "\n",
    "    for embed in soup.find_all('embed', src=True):\n",
    "        dots = [x.start() for x in re.finditer(r'\\.', embed['src'])]\n",
    "        if wiki in embed['src'] or domain in embed['src'] or len(dots) == 1:\n",
    "            success = success + 1\n",
    "        i = i + 1\n",
    "\n",
    "    for i_frame in soup.find_all('i_frame', src=True):\n",
    "        dots = [x.start() for x in re.finditer(r'\\.', i_frame['src'])]\n",
    "        if wiki in i_frame['src'] or domain in i_frame['src'] or len(dots) == 1:\n",
    "            success = success + 1\n",
    "        i = i + 1\n",
    "\n",
    "    try:\n",
    "        percentage = success / float(i) * 100\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "    if percentage < 22.0:\n",
    "        return 1\n",
    "    elif 22.0 <= percentage < 61.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def url_of_anchor(wiki, soup, domain):\n",
    "    i = 0\n",
    "    unsafe = 0\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        # 2nd condition was 'JavaScript ::void(0)' but we put JavaScript because the space between javascript and ::\n",
    "        # might not be\n",
    "        # there in the actual a['href']\n",
    "        if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (\n",
    "                wiki in a['href'] or domain in a['href']):\n",
    "            unsafe = unsafe + 1\n",
    "        i = i + 1\n",
    "        # print a['href']\n",
    "    try:\n",
    "        percentage = unsafe / float(i) * 100\n",
    "    except:\n",
    "        return 1\n",
    "    if percentage < 31.0:\n",
    "        return 1\n",
    "        # return percentage\n",
    "    elif 31.0 <= percentage < 67.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Links in <Script> and <Link> tags\n",
    "def links_in_tags(wiki, soup, domain):\n",
    "    i = 0\n",
    "    success = 0\n",
    "    for link in soup.find_all('link', href=True):\n",
    "        dots = [x.start() for x in re.finditer(r'\\.', link['href'])]\n",
    "        if wiki in link['href'] or domain in link['href'] or len(dots) == 1:\n",
    "            success = success + 1\n",
    "        i = i + 1\n",
    "\n",
    "    for script in soup.find_all('script', src=True):\n",
    "        dots = [x.start() for x in re.finditer(r'\\.', script['src'])]\n",
    "        if wiki in script['src'] or domain in script['src'] or len(dots) == 1:\n",
    "            success = success + 1\n",
    "        i = i + 1\n",
    "    try:\n",
    "        percentage = success / float(i) * 100\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "    if percentage < 17.0:\n",
    "        return 1\n",
    "    elif 17.0 <= percentage < 81.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Server Form Handler (SFH)\n",
    "# Have written conditions directly from word file..as there are no sites to test ######\n",
    "def sfh(wiki, soup, domain):\n",
    "    for form in soup.find_all('form', action=True):\n",
    "        if form['action'] == \"\" or form['action'] == \"about:blank\":\n",
    "            return -1\n",
    "        elif wiki not in form['action'] and domain not in form['action']:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    return 1\n",
    "\n",
    "\n",
    "# Mail Function\n",
    "# PHP mail() function is difficult to retrieve, hence the following function is based on mailto\n",
    "def submitting_to_email(soup):\n",
    "    for form in soup.find_all('form', action=True):\n",
    "        return -1 if \"mailto:\" in form['action'] else 1\n",
    "    # In case there is no form in the soup, then it is safe to return 1.\n",
    "    return 1\n",
    "\n",
    "\n",
    "def abnormal_url(domain, url):\n",
    "    hostname = domain.name\n",
    "    match = re.search(hostname, url)\n",
    "    return 1 if match else -1\n",
    "\n",
    "\n",
    "# IFrame Redirection\n",
    "def i_frame(soup):\n",
    "    for i_frame in soup.find_all('i_frame', width=True, height=True, frameBorder=True):\n",
    "        # Even if one iFrame satisfies the below conditions, it is safe to return -1 for this method.\n",
    "        if i_frame['width'] == \"0\" and i_frame['height'] == \"0\" and i_frame['frameBorder'] == \"0\":\n",
    "            return -1\n",
    "        if i_frame['width'] == \"0\" or i_frame['height'] == \"0\" or i_frame['frameBorder'] == \"0\":\n",
    "            return 0\n",
    "    # If none of the iframes have a width or height of zero or a frameBorder of size 0, then it is safe to return 1.\n",
    "    return 1\n",
    "\n",
    "\n",
    "def age_of_domain(domain):\n",
    "    creation_date = domain.creation_date\n",
    "    expiration_date = domain.expiration_date\n",
    "    ageofdomain = 0\n",
    "    if expiration_date:\n",
    "        ageofdomain = abs((expiration_date - creation_date).days)\n",
    "    return -1 if ageofdomain / 30 < 6 else 1\n",
    "\n",
    "\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        rank = \\\n",
    "            bs4.BeautifulSoup(urllib.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\n",
    "                \"REACH\")['RANK']\n",
    "    except TypeError:\n",
    "        return -1\n",
    "    rank = int(rank)\n",
    "    return 1 if rank < 100000 else 0\n",
    "\n",
    "\n",
    "def google_index(url):\n",
    "    site = search(url, 5)\n",
    "    return 1 if site else -1\n",
    "\n",
    "\n",
    "def statistical_report(url, hostname):\n",
    "    try:\n",
    "        ip_address = socket.gethostbyname(hostname)\n",
    "    except:\n",
    "        return -1\n",
    "    url_match = re.search(\n",
    "        r'at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly', url)\n",
    "    ip_match = re.search(\n",
    "        '146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "        '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "        '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "        '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "        '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "        '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',\n",
    "        ip_address)\n",
    "    if url_match:\n",
    "        return -1\n",
    "    elif ip_match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_hostname_from_url(url):\n",
    "    hostname = url\n",
    "    # TODO: Put this pattern in patterns.py as something like - get_hostname_pattern.\n",
    "    pattern = \"https://|http://|www.|https://www.|http://www.\"\n",
    "    pre_pattern_match = re.search(pattern, hostname)\n",
    "\n",
    "    if pre_pattern_match:\n",
    "        hostname = hostname[pre_pattern_match.end():]\n",
    "        post_pattern_match = re.search(\"/\", hostname)\n",
    "        if post_pattern_match:\n",
    "            hostname = hostname[:post_pattern_match.start()]\n",
    "\n",
    "    return hostname\n",
    "\n",
    "# TODO: Put the DNS and domain code into a function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url):\n",
    "    with open(LOCALHOST_PATH + DIRECTORY_NAME + '/markup.txt', 'r') as file:\n",
    "        soup_string = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(soup_string, 'html.parser')\n",
    "\n",
    "    status = []\n",
    "    hostname = get_hostname_from_url(url)\n",
    "\n",
    "    status.append(having_ip_address(url))\n",
    "    status.append(url_length(url))\n",
    "    status.append(shortening_service(url))\n",
    "    status.append(having_at_symbol(url))\n",
    "    status.append(double_slash_redirecting(url))\n",
    "    status.append(prefix_suffix(hostname))\n",
    "    status.append(having_sub_domain(url))\n",
    "\n",
    "    dns = 1\n",
    "    try:\n",
    "        domain = whois.query(hostname)\n",
    "    except:\n",
    "        dns = -1\n",
    "\n",
    "    status.append(-1 if dns == -1 else domain_registration_length(domain))\n",
    "\n",
    "    status.append(favicon(url, soup, hostname))\n",
    "    status.append(https_token(url))\n",
    "    status.append(request_url(url, soup, hostname))\n",
    "    status.append(url_of_anchor(url, soup, hostname))\n",
    "    status.append(links_in_tags(url, soup, hostname))\n",
    "    status.append(sfh(url, soup, hostname))\n",
    "    status.append(submitting_to_email(soup))\n",
    "\n",
    "    status.append(-1 if dns == -1 else abnormal_url(domain, url))\n",
    "\n",
    "    status.append(i_frame(soup))\n",
    "\n",
    "    status.append(-1 if dns == -1 else age_of_domain(domain))\n",
    "\n",
    "    status.append(dns)\n",
    "\n",
    "    status.append(web_traffic(soup))\n",
    "    status.append(google_index(url))\n",
    "    status.append(statistical_report(url, hostname))\n",
    "\n",
    "    print('\\n1. Having IP address\\n2. URL Length\\n3. URL Shortening service\\n4. Having @ symbol\\n'\n",
    "          '5. Having double slash\\n6. Having dash symbol(Prefix Suffix)\\n7. Having multiple subdomains\\n'\n",
    "          '8. SSL Final State\\n8. Domain Registration Length\\n9. Favicon\\n10. HTTP or HTTPS token in domain name\\n'\n",
    "          '11. Request URL\\n12. URL of Anchor\\n13. Links in tags\\n14. SFH\\n15. Submitting to email\\n16. Abnormal URL\\n'\n",
    "          '17. IFrame\\n18. Age of Domain\\n19. DNS Record\\n20. Web Traffic\\n21. Google Index\\n22. Statistical Reports\\n')\n",
    "    print(status)\n",
    "    return status\n",
    "\n",
    "\n",
    "# Use the below two lines if features_extraction.py is being run as a standalone file. If you are running this file as\n",
    "# a part of the workflow pipeline starting with the chrome extension, comment out these two lines.\n",
    "# if __name__ == \"__main__\":\n",
    "#     if len(sys.argv) != 2:\n",
    "#         print(\"Please use the following format for the command - `python2 features_extraction.py <url-to-be-tested>`\")\n",
    "#         exit(0)\n",
    "#     main(sys.argv[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
